\section{Conseguir mais em menos tempo}

	A computação sempre foi uma das principais ferramentas humanas para avanços 
	tecnológicos nos últimos séculos. Seja na engenharia, medicina, área 
	militar, biologia, química, sísmica e até no cinema, os computadores tem 
	sido utilizados em tarefas como mecânicas, estudos patológicos, 
	ataques/defesas nacionais, enovelamento de proteínas, dinâmica molecular, 
	monitoramento sísmico e animações tridimensionais.
	
	Todas essas áreas costumam requerer que os resultados obtidos 
	computacionalmente sejam gerados o mais rápido possível. Além disso, 
	muitas (senão quase todas) as \textbf{simulações científicas}\footnote{
		explicar o que seriam simulações científicas}
	envolvidas não podem ser executadas em tempo hábil com o tipo de processador
	visto na Seção \ref{sec:processor}. Visto 
	isso, os projetistas por trás dos projetos de processadores precisaram 
	implementar mecanismos nos modelos para explorá-los ao máximo. Nessa tarefa, 
	encontraram barreiras \todo{Ir à biblioteca}\cite{autor:intro-par-comp}.
    
    \subsection{A barreira do paralelismo a nível de instruções - \textit{ILP wall}}
    
    	Vimos na Seção \ref{sec:processor} um processador com a capacidade de 
    	executar uma instrução por ciclo de \textit{clock}. Com isso, a duração 
    	do ciclo devia ser a mesma da execução da instrução mais demorada. 
    	Na necessidade de se adquirir mais velocidade de processamento, 
    	os projetistas perceberam que podiam particionar as instruções, de modo 
    	a executar cada estágio resultante em um ciclo de \textit{clock}, 
    	deixando o resultado para a próxima partição a ser operada. Assim 
    	nasceu o processador \textbf{multiciclo}. Dessa forma, a duração 
    	do ciclo de \textit{clock} reduziu para a mesma do estágio mais 
    	demorado dentre as instruções.
    	
    	Contudo, a necessidade de se acelerar os processadores continuava. Em
    	seguida, os projetistas perceberam que as unidades funcionais dos 
    	processadores ficavam ociosas quando não se tratava do estágio de uma 
    	instrução em que elas eram utilizadas. Era então possível executar uma
    	instrução ao mesmo tempo que outra, desde que ambas se encontrassem, 
    	cada uma, em estágios $A$ e $B$, sendo $A$ o estágio da instrução mais 
    	antiga e $B$ o da mais nova,  com $B = A - 1$. 
    	
    	Ou seja, por exemplo, 
    	considere uma instrução $i$ liberada no tempo de \textit{clock} $1$, 
    	passando por seu primeiro estágio. No tempo $2$, ela estará em sua
    	segunda etapa e as unidades funcionais responsáveis pela primeira 
    	estariam ociosas, se não fosse pela inovação apresentada acima. Com 
    	esta, uma nova instrução $j$ é buscada ainda nesse tempo, tendo 
    	seu primeiro estágio executado. No tempo de ciclo de \textit{clock} 3,
    	a instrução $i$ passará para o seu terceiro estágio, enquanto a $j$
    	passará para o segundo e uma nova instrução poderá ser buscada. Ao 
    	``trem'' de estágios foi dado o nome de \textbf{\textit{pipeline}}. À essa 
    	inovação, foi dado o nome de \textbf{paralelismo a nível de instruções}, 
    	visto que se tem mais de uma instrução sendo executada ao mesmo tempo e 
    	uma pronta a cada ciclo de \textit{clock}.
    	
    	Em seguida, os projetistas decidiram construir estágios menores, 
    	consequentemente aumentando a frequência de \textit{clock} (que é 
	    	$\frac{1}{\text{tempo de ciclo do \textit{clock}}}$ 
    	e dada em hertz [Hz]), o tamanho do \textit{pipeline} e o número de 
    	instruções operadas simultaneamente. Nasceu o \textbf{superpipeline}. 
    	
    	Apesar disso aumentar a performance dos processadores, ao se 
    	incrementar o número de estágios para um valor maior que \todo{buscar fonte e confirmação} oito tem-se
    	o desempenho degradado.
    	
    	Por fim, os projetistas de \textit{hardware} ainda deram mais um 
    	passo em busca de mais performance: a \textbf{superescalaridade}, 
    	que consiste, basicamente em \textit{pipelines} em paralelo, o que 
    	propicia a execução de múltiplas instruções ao mesmo tempo. Contudo, como é
    	costumeiro, essa arquitetura também apresentou seus defeitos. Os 
    	\textit{pipelines} em paralelo leva a problemas para acessar os recursos comuns, 
    	como memória, por exemplo, sendo necessário duplicá-los. Além disso, 
    	a ocorrência de \textbf{dependências de dados}\footnote{\textbf{dependências de dados} 
    	são eventos problemáticos que ocorrem quando duas ou mais instruções utilizam um mesmo 
    	recurso (um registrador, por exemplo) de forma que a primeira instrução não pode 
    	ler/escrever sobre esse recurso sem que a segunda (ou qualquer ordem próxima) já o 
    	tenha feito (ou vice-versa), de forma que não venha a ter interferência na coesão dos 
    	dados \cite{citar alguma referencia aqui}.} e 
    	\textbf{controle}\footnote{\textbf{dependências de controle}, semelhantemente às de 
    	dados, são situações em que uma instrução precisa que uma de suas anteriores leve a um 
    	resultado que permita a sua execução, como na estrutura 
    	\lstinline[columns=fixed]{if(...)\{...\}} na linguagem \texttt{C}, cujo código 
    	presente dentro das chaves só poderá ser executado se a condição dentro dos parênteses 
    	for verdadeira \cite{wiki:dependencies}.} fazem com que em alguns ciclos não hajam 
    	instruções para serem executadas, visto o atraso necessário para que as dependências 
    	sejam resolvidas \cite{slide:adv-microarch}.
    
    \subsection{A barreira no gasto de energia dos processadores - \textit{Power wall}}
    
    	Em 1965, Gordon Moore publicou um artigo em que descrevia uma observação de que o
    	número de componentes por \textbf{circuito integrado}\footnote{explicar o que é um 
    	\textbf{circuito integrado}}, \textbf{transistores}\footnote{explicar o que são 
    	\textbf{transistores}} no caso, dobrava a cada dois anos. Ele então estimou que essa 
	    taxa de crescimento deveria continuar por pelo menos uma década. O período dessa 
	    taxa veio então a ser alterada depois para um ano e meio e a estimativa se manteve 
	    firme por muitas décadas \cite{wiki:moorelaw}.
    	
    	O crescimento da densidade desses componentes permitiu aos processadores alcançarem um taxa muito mais alta de \textit{clock}.
    
        \begin{itemize}
            \item Introduzir a lei de Moore;
            \item Explicar que houve evolução na taxa de clock ao longo do tempo;
            \item Explicar que essa evolução foi amortecida nos últimos anos devido 
            ao gasto energético e às altas temperaturas que os processadores alcançaram
        \end{itemize}
    
    \subsection{A barreira da memória - \textit{Memory wall}}
    
	    \todo[inline]{Conferir se existe mesmo a memory wall}
	    \todo[inline]{Revisar o que é a memory wall e completar o itemize abaixo}
	    
	    \begin{itemize}
	    	\item 
	    \end{itemize}