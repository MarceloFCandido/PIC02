\section{Paralelismo - a alternativa para se contornar as barreiras}

	\label{sec:parallelism}
    
    Visto a incapacidade de se transpor o alto gasto energético das 
    unidades de processamento, junto com as consequentes altas temperaturas,
    além da incapacidade de se aumentar o número de instruções prontas por 
    ciclo de \textit{clock}, precisamos de uma alternativa para se continuar 
    aumentando o poder de processamento dos computadores. 
    
    Tal alternativa foi então aumentar o número de unidades de processamento,
    seja em um \textit{chip} com mais de uma unidade (chamada núcleo, ou 
    \textbf{\textit{core}} em inglês) ou em um sistema com mais de uma 
    máquina, que por sua vez possui uma ou mais unidades de processamento. 
    Dessa forma, o número de tarefas concluídas por unidade de tempo aumentou,
    visto.
    A isso foi dado o nome de \textbf{paralelismo}.
    
    Existem muitos motivos para se utilizar o paralelismo. O mais importante é
    que o universo possui muitos processos paralelos, ou seja, ocorrendo ao 
    mesmo tempo, tais como mudanças climáticas, montagens de veículos e 
    aeronaves, tráfico em um pedágio e acessos a um site. Em consequência da 
    necessidade de se processar esses eventos por computadores, surgem outros motivos para a computação paralela:
    \begin{itemize}
    	\item poupar tempo e/ou dinheiro: existem problemas computacionais 
    	que são muito demorados, senão impossíveis, para se resolver
    	serialmente. Dessa forma, usando-se a computação serial, se paga mais,
    	visto o uso por mais tempo do poder computacional;
    	\item resolver problemas maiores e/ou mais complexos, visto o
    	processamento mais rápido e a possibilidade de se dividir o trabalho
    	mais máquinas;
    	\item prover concorrência: realizar várias tarefas simultaneamente
    	\cite{LLNL:parcomp}.
    \end{itemize}
    
    Quando se fala em paralelismo, é importante mostrar que existem diferentes
    disposições de memória nesse meio, o que veremos na subseção
    \ref{subsec:memory-architectures}. Além disso, existem diferentes
    paradigmas de paralelismo, sendo alguns brevemente explicados na subseção
    \ref{subsec:par-comp-models}. Há também outros assuntos a serem tratados 
    ao se projetar programas paralelos, sendo alguns abordados na subseção 
    \ref{subsec:parallel-design}. Fora isso, mais informações sobre 
    paralelismo podem ser encontradas em Barney \cite{LLNL:parcomp}.
    
    \subsection{Arquiteturas de memória na computação paralela}
    
	    \label{subsec:memory-architectures}
	    
	    Na computação paralela, existem diferentes formas pelas quais a 
	    memória é administrada e interpretada. Isso pode ser realizado fisica 
	    e/ou logicamente, principalmente de três formas:
        \begin{itemize}
            \item memória compartilhada: cada unidade de processamento de
            um sistema tem acesso à toda a memória, com um \textbf{espaço de
            endereçamento}\footnote{\textbf{espaço de endereçamento}, segundo
            a Wikipédia \cite{wiki:address}, é uma ``série de endereços
            discretos'' que, no caso, nomeiam as posições de memória} global;
            
            \item memória distribuída: cada unidade de processamento possui
            sua própria memória, mapeada apenas para ele e que pode ser 
            acessada pelos demais nós através de requisições feitas por meio
            de uma rede que os liga. Além disso, cada nó opera
            independentemente, visto que cada um tem sua própria memória.
            Dessa forma, as mudanças que um nó opera sobre sua própria 
            memória não afetam as dos demais nós;
            
            \item híbrida: literalmente a mescla de ambas, ou seja, cada 
            \textbf{nó}\footnote{\textbf{nó} é o nome dado na computação 
            paralela para cada unidade de processamento (ou conjunto dessas) 
            independente.} possui mais de uma unidade de processamento e sua 
	        própria memória, sendo também capaz de acessar as dos outros 
	        \cite{LLNL:parcomp}.
        \end{itemize}
        
    
    \subsection{Modelos da computação paralela}
    
		\label{subsec:par-comp-models}
		Os modelos de programação paralela 
    
        \begin{itemize}
            \item Shared Memory Model;
            \item Threads Model;
            \item Distributed Memory / Message Passing Model;
            \item Data Parallel Model;
            \item Hybrid Model
        \end{itemize}
    
    \subsection{Desenhando programas paralelos}
    
	    \label{subsec:parallel-design}
    
        \begin{itemize}
            \item explicar a diferença entre a paralelização automática e a 
            manual;
            \item explicar a necessidade de se entender o problema e o programa;
            \item explicar
            \begin{itemize}
            	\item particionamento;
            	\item sincronização.
            \end{itemize}

        \end{itemize}