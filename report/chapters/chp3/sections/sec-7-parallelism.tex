\section{Paralelismo - a alternativa para se contornar as barreiras}

	\label{sec:parallelism}
    
    Visto a incapacidade de se transpor o alto gasto energético das 
    unidades de processamento, junto com as consequentes altas temperaturas,
    além da incapacidade de se aumentar o número de instruções prontas por 
    ciclo de \textit{clock}, precisamos de uma alternativa para se continuar 
    aumentando o poder de processamento dos computadores. 
    
    Tal alternativa foi então aumentar o número de unidades de processamento,
    seja em um \textit{chip} com mais de uma unidade (chamada núcleo, ou 
    \textbf{\textit{core}} em inglês) ou em um sistema com mais de uma 
    máquina, que por sua vez possui uma ou mais unidades de processamento. 
    Dessa forma, o número de tarefas concluídas por unidade de tempo aumentou,
    visto.
    A isso foi dado o nome de \textbf{paralelismo}.
    
    Existem muitos motivos para se utilizar o paralelismo. O mais importante é
    que o universo possui muitos processos paralelos, ou seja, ocorrendo ao 
    mesmo tempo, tais como mudanças climáticas, montagens de veículos e 
    aeronaves, tráfico em um pedágio e acessos a um site. Em consequência da 
    necessidade de se processar esses eventos por computadores, surgem outros motivos para a computação paralela:
    \begin{itemize}
    	\item poupar tempo e/ou dinheiro: existem problemas computacionais 
    	que são muito demorados, senão impossíveis, para se resolver
    	serialmente. Dessa forma, usando-se a computação serial, se paga mais,
    	visto o uso por mais tempo do poder computacional;
    	\item resolver problemas maiores e/ou mais complexos, visto o
    	processamento mais rápido e a possibilidade de se dividir o trabalho
    	mais máquinas;
    	\item prover concorrência: realizar várias tarefas simultaneamente
    	\cite{LLNL:parcomp}.
    \end{itemize}
    
    Quando se fala em paralelismo, é importante mostrar que existem diferentes
    disposições de memória nesse meio, o que veremos na subseção
    \ref{subsec:memory-architectures}. Além disso, existem diferentes
    paradigmas de paralelismo, sendo alguns brevemente explicados na subseção
    \ref{subsec:par-comp-models}. Há também outros assuntos a serem tratados 
    ao se projetar programas paralelos, sendo alguns abordados na subseção 
    \ref{subsec:parallel-design}. Fora isso, mais informações sobre 
    paralelismo podem ser encontradas em Barney \cite{LLNL:parcomp}.
    
    \subsection{Arquiteturas de memória na computação paralela}
    
	    \label{subsec:memory-architectures}
	    
	    Na computação paralela, existem diferentes formas pelas quais a 
	    memória é implementada em um sistema. No caso deste trabalho, são
	    elucidadas as três seguintes:
        \begin{itemize}
            \item memória compartilhada: cada unidade de processamento de
            um sistema tem acesso à toda a memória, com um \textbf{espaço de
            endereçamento}\footnote{\textbf{espaço de endereçamento}, segundo
            a Wikipédia \footcite{wiki:address}, é uma ``série de endereços
            discretos'' que, no caso, nomeiam as posições de memória} global;
            
            \item memória distribuída: cada unidade de processamento possui
            sua própria memória, mapeada apenas para ele e que pode ser 
            acessada pelos demais nós através de requisições feitas por meio
            de uma rede que os liga. Além disso, cada nó opera
            independentemente, visto que cada um tem sua própria memória.
            Dessa forma, as mudanças que um nó opera sobre sua própria 
            memória não afetam as dos demais nós;
            
            \item híbrida: literalmente a mescla de ambas, ou seja, cada 
            \textbf{nó}\footnote{\textbf{nó} é o nome dado na computação 
            paralela para cada unidade de processamento (ou conjunto dessas) 
            independente.} possui mais de uma unidade de processamento e sua 
	        própria memória, sendo também capaz de acessar as dos outros 
	        \cite{LLNL:parcomp}.
        \end{itemize}
        
        Essas arquiteturas são implementadas fisicamente nos sistemas de 
        computação paralela, contudo, o programador pode escolher interpretar 
        a arquitetura do sistema, com o qual ele trabalha, de forma diferente
        ou não \cite{LLNL:parcomp}. Isso é o que veremos na próxima seção.
    
    \subsection{Modelos da computação paralela}
    
		\label{subsec:par-comp-models}
		
		Segundo Barney, os modelos de programação paralela funcionam ``como
		uma abstração sobre o \textit{hardware} e as arquiteturas de memória''
		(tradução nossa), ou seja, eles servem ao programador como formas de
		se mascarar (ou não) a estrutura física implementada para um dado 
		sistema de computação paralela. 
		
		Isso quer dizer que, teoricamente, se você tem um sistema com memória
		distribuída que será vista pelo usuário (mediante implementação) como
		compartilhada. Esse foi o caso, por exemplo, do supercomputador
		\textit{Kendall Square Research} (KSR). Semelhantemente, um sistema 
		com memória compartilhada poderia ser interpretado como um de memória 
		distribuída \cite{LLNL:parcomp}.
		
		Uma lista com modelos de computação paralela diretamente relacionados 
		com esse trabalho é dada abaixo:    
        \begin{itemize}
            \item modelo de memória compartilhada (sem
            \textit{\textbf{threads}}\footnote{de forma básica, uma
            \textit{\textbf{thread}} pode ser entendida como uma tarefa 
            (no inglês, \textit{task}), composta por instruções e lançada por 
            um programa, a ser executada pelo sistema operacional de forma 
            independente (concorrente) a quem a lançou. Essa independência
            permite, por exemplo, que várias \textit{threads} sejam lançadas
            e executem de forma independente entre si\footcite{LLNL:pthreads}.}): 
	        consiste em processos/tarefas compartilhando o mesmo espaço de 
	        endereçamento, onde eles podem ler e escrever assincronamente. 
	        Como vantagem, esse modelo não possui o conceito de posse de 
	        dados, o que torna desnecessária a explicitação de como os dados 
	        devem ser comunicados entre as tarefas. Como desvantagem, existe
	        a necessidade de se controlar a \todo{não deixar de explicar esse conceito na seção da memória} localidade dos dados;
            
            \item modelo de \textit{threads}: é também um tipo de programação 
            com memória compartilhada que consiste em um programa principal 
            que lança várias linhas de execução concorrentes com instruções 
            chamadas \textit{threads}. Essas últimas são designadas e 
            executadas simultaneamente, sendo desse fato que vem o 
            paralelismo desse modelo. Algumas 
            \textbf{API's}\footnote{\textbf{API}, do inglês 
            \textit{Application Programming Interface} (Interface de 
            Programação de Aplicação) ``é um conjunto de definições de 
            subrotinas, protocolos de comunicação e ferramentas para a 
            construção de \textit{software}'' \footcite{wiki:API}.}
            que usam esse modelo são \texttt{OpenMP} (Multiprocessamento Aberto, do inglês \textit{Open Multi-Processing}) 
            e \texttt{Pthreads} (POSIX\footnote{Interface Portável entre 
            Sistemas Operacionais, do inglês \textit{Portable Operating 
            System Interface} \cite{wiki:POSIX}.} Threads);
            
            \item modelo de memória distribuída / troca de mensagens: 
            consiste em um conjunto de tarefas, que podem residir no mesmo
            espaço de endereçamento e/ou ao longo de um número arbitrário de 
            máquinas, usando a memória local durante a computação. Além disso,
            as tarefas precisam mandar/receber dados de outras tarefas, o que
            é feito através do envio/recebimento de mensagens. Uma API que 
            implementa esse modelo é a \texttt{MPI} (Message Passing 
            Interface);
            
            \item modelo híbrido: mescla modelos acima, sendo um exemplo a 
            aplicação da MPI para a comunicação entre as máquinas de um 
            sistema e alguma API de \textit{threads} para a realização das 
            computações dentro de cada máquina, que é um subsistema de 
            memória compartilhada.
            
        \end{itemize}
    
    \subsection{Desenhando programas paralelos}
    
	    \label{subsec:parallel-design}
	    
 	    Para se desenhar programas paralelos para esse trabalho, mais alguns 
 	    conceitos, detalhes e diferenças devem ser explicados. Um deles é a diferença 
 	    entre a paralelização manual e a automática. A manual 
 	    se dá com o programador (preferencialmente usando uma API) 
 	    determinando o local e o tempo ou da criação de \textit{threads}, ou
 	    do envio/recebimento de mensagens ou de qualquer outra ação 
 	    relacionada; por si só, sem automação. Já a automática ou é realizada 
 	    por um compilador/pré-processador, que identifica as regiões do 
 	    código que podem ser paralelizadas; ou pelo programador, que seta 
 	    \textbf{\textit{flags}}\footnote{\textbf{\textit{flag}} 
   		(bandeira, no português), é um termo utilizado na computação para 
   		designar mecanismos que servem como indicadores para outros agentes 
   		de um mesmo sistema.} ou diretivas para indicar ao compilador as 
    	partes do código a serem paralelizadas.
	    
	    Outro detalhe importante é a necessidade de se entender o problema a 
	    se tentar paralelizar e o programa que o modela. Entender o programa 
	    é necessário para se saber se ele é paralelizável. Caso seja, o 
	    próximo passo seria construir um \textbf{programa serial}\footnote{Um 
	    \textbf{programa serial} é aquele que seus comandos são executados 
	    em ordem, sem paralelismo.} que o resolva. A partir daí, é necessário 
		que se entenda esse programa a fim de que se descubra os pontos onde 
		ele leva mais tempo e a paralelização seria mais eficiente; e os 
		pontos de gargalo, onde a execução desacelera desproporcionalmente e a
		paralelização se torna ineficiente. Pode também ser necessário 
		reestruturar o programa ou até procurar outro algoritmo que o resolva.
		
		Por fim, é preciso elucidar dois conceitos a serem utilizados nessa 
		iniciação:
        \begin{itemize}
           	\item particionamento: consiste em quebrar um problema (a ser resolvido por uma abordagem paralela) em partes discretas para 
           	serem distribuídas a tarefas. Isso pode ser feito de duas formas: \begin{enumerate}
           		\item particionamento de domínio: o conjunto de dados a ser 
           		processado é decomposto em partes e cada subconjunto é 
           		processado;
           		
           		\item particionamento funcional: distribuição do trabalho a 
           		ser realizado a tarefas. Esse é o tipo de particionamento a 
           		ser utilizado nessa iniciação.
           	\end{enumerate}
           	
           	\item sincronização: relacionada com o problema de se gerenciar 
           	a sequência do trabalho e das tarefas, se manifesta em diferentes 
           	tipos como trava/semáforo, operações síncronas de comunicação e 
           	``barreira''. Esse último conceito, que será utilizado nesse 
           	trabalho, consiste em cada tarefa trabalhar até atingir um 
           	determinado ponto do programa (a barreira), onde elas são 
           	bloqueadas. Quando a última tarefa chega a esse ponto, o processo 
           	está sincronizado e o que acontece a partir daí fica a cargo do 
           	programador.
        \end{itemize}