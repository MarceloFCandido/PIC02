Parallel Computing - LLNL - Just an abstract

Serial computation
    By tradition, we used to write programs to solve problems by a serial
    (sequential) way for execution in a single processor. It means, the
    instructions of these programs were executed by one at time.
Parallel computation
    In the parallel approach, we get the problem that has to be solved and separate
    it in $n$ parts. So, we get this parts and give to $n$ different
    processors. For this, we employ a control/coordination mechanism. It
    often takes less time to solve the given problem in comparison with
    the serial approach.

The parallel approach needs
    - compatible resources (parallel computers, computers connected in a
      network, etc);
    - a compatible problem (that can be broken in parts, get multiple
        instructions executed simutaneously, etc).

Why use parallel computing?
    - "Compared to serial computing, parallel computing is much better suited
      for modeling, simulating and understanding complex, real world phenomena."
    - "save time and/or money"
    - "solve larger/more complex problems"
    - "do many things simultaneously"
    - better usage of non-local resources and parallel hardware

The von Neumann Architecture
    Create by the hungarian mathematician Jonh von Neumann, it was designed
    to keep data and program instructions in a electronic memory, instead of
    a scheme of "hard wiring" of the earlier computers. Because of this
    architecture, the subsequent computers from 1945 (year of publication of
    von Neumann Architecture) to today have as componnents (but not just these)
        - memory (stores program instructions and data);
        - the processor (with the next two modules as part of this)
            - control unit (operates above instructions, fetching, decodifying
            and coodinating);
            - arithmetic logic unit (that performs basic arithmetic operations);
        - input/output (interface to human operators).
    "(...) Parallel computers still follow this basic design, just multiplied
    in units. The basic, fundamental architecture remains the same."

The Flynn's Taxonomy
    Used to classify parallel computers since 1966 by "two independent
    dimensions of Instruction Stream and Data Stream", each one with only two
    possible stages: Single and Multiple.
        - SISD (Single Instruction, Single Data) (non-parallel)
            One instruction being executed at CPU during one clock cycle and
            one data stream being used as input any one single clock cycle, like
            in a serial computer, with deterministic execution
        - SIMD (Single Instruction, Multiple Data)
            It executes by a synchronous and derteministic way, where all
            processing units execute the same instruction, but each one can use
            a different data element
        - MISD (Multiple Instruction, Single Data)
            It works by each processing unit using using the same data via
            separated instruction streams.
        - MIMD (Multiple Instruction, Multiple Data)
            Being the most common parallel architecture in these days, it can be
            built by others SIMD modules. In the model, each processing unit can
            use different instructions and data streams from the others.

Some General Parallel Terminology (just the unknow or most important)
    "CPUs with multiple cores are sometimes called 'sockets'"
    Task
        "A logically discrete section of computational work. A task is typically
        a program or program-like set of instructions that is executed by a
        processor. A parallel program consists of multiple tasks running on
        multiple processors."
    Pipelining (different meaning from the commom computer architecture)
        "Breaking a task into steps performed by different processor units, with
        inputs streaming through, much like an assembly line; a type of parallel
        computing."
    Shared Memory
        "From a strictly hardware point of view, describes a computer
        architecture where all processors have direct (usually bus based) access
        to common physical memory. In a programming sense, it describes a model
        where parallel tasks all have the same "picture" of memory and can
        directly address and access the same logical memory locations regardless
        of where the physical memory actually exists."
    Symmetric Multi-Processor (SMP)
        "Shared memory hardware architecture where multiple processors share a
        single address space and have equal access to all resources."
    Distributed Memory
        "In hardware, refers to network based memory access for physical memory
        that is not common. As a programming model, tasks can only logically
        "see" local machine memory and must use communications to access memory
        on other machines where other tasks are executing."
    Communications
        "Parallel tasks typically need to exchange data. There are several ways
        this can be accomplished, such as through a shared memory bus or over a
        network, however the actual event of data exchange is commonly referred
        to as communications regardless of the method employed."
    Parallel Overhead
        "The amount of time required to coordinate parallel tasks, as opposed to
        doing useful work. Parallel overhead can include factors such as:
            Task start-up time
            Synchronizations
            Data communications
            Software overhead imposed by parallel languages, libraries,
            operating system, etc.
            Task termination time"
    Synchronization
	"The coordination of parallel tasks in real time, very often associated
	with communications. Often implemented by establishing a synchronization
        point within an application where a task may not proceed further until
        another task(s) reaches the same or logically equivalent point.
        Synchronization usually involves waiting by at least one task, and can
        therefore cause a parallel application's wall clock execution time to
        increase."
    Granularity
	    "In parallel computing, granularity is a qualitative measure of the
	    ratio of computation to communication.
		    - Coarse: relatively large amounts of computational work are done
		    between communication events
		    - Fine: relatively small amounts of computational work are done
		    between communication events"
    Observed Speedup
        "Observed speedup of a code which has been parallelized, defined as:
            wall-clock time of serial execution
            -----------------------------------
           wall-clock time of parallel execution
        One of the simplest and most widely used indicators for a parallel
        program's performance."
    Massively Parallel
        "Refers to the hardware that comprises a given parallel system - having
        many processing elements. The meaning of "many" keeps increasing, but
        currently, the largest parallel computers are comprised of processing
        elements numbering in the hundreds of thousands to millions."
    Embarrassingly Parallel
        "Solving many similar, but independent tasks simultaneously; little to
        no need for coordination between the tasks."
    Scalability
        "Refers to a parallel system's (hardware and/or software) ability to
        demonstrate a proportionate increase in parallel speedup with the
        addition of more resources. Factors that contribute to scalability
        include:
            - Hardware - particularly memory-cpu bandwidths and network
            communication properties
            - Application algorithm
            - Parallel overhead related
            - Characteristics of your specific application"

Limits and costs of parallel programming
    Amdahl's Law
        It intents to measure the speedup involved in a program that
        has a porcentage $P$ of it parallelized by the formula
                                            1
                                speedup = -----
                                          1 - P
        We can also calculate the speedup involving the number $N$ of processors
        that can execute the parallel part of code simutaneously, excepting the
        serial porcentage $S$ of code.
                                            1
                                speedup = -----
                                          P - S
                                         ---
                                          N
        The speedup by scalability has a limit. But this limit can be increased
        by increasing the problem size (for some problems). This type of problem
        is more scalable than the problems "with a fixed percentage of parallel
        time".

    Complexity
        "In general, parallel applications are much more complex than
        corresponding serial applications, perhaps an order of magnitude. Not
        only do you have multiple instruction streams executing at the same
        time, but you also have data flowing between them."

    Resource Requirements
        "For short running parallel programs, there can actually be a decrease
        in performance compared to a similar serial implementation. The
        overhead costs associated with setting up the parallel environment,
        task creation, communications and task termination can comprise a
        significant portion of the total execution time for short runs."

    Scalability
        Strong scaling
            Run a program faster increasing the number of processors that work
            on it
        Weak scaling
            You give to all the processors involved the same amount of work and
            the problem will run always in the same amount of time (at least, in
            theory). The problem size must be proportional to the number of
            processors.
        As said above, the scalability has limits and going over this limits
        can result in decreasing of the performance.

Parallel Computer Memory Architectures
    Shared memory
        It consists in processors that can operate independently but sharing the
        same memory resources. It can be classified in
            - Uniform Memory Access (UMA): with identical processors, each of
            this has equal access and access time to the memory;
            - Non-uniform Memory Access (NUMA): generaly, it is built by UMAs
            linked, in a way that a UMA can access memory of other (this type
            of access is slower). In this one, "not all the processors have
            equal access time to all memories".

        - Advantages: "user-friendly programming perspective to memory" and fast
        and uniform data-sharing between tasks.
        - Disadvantages: "lack of scalability between memory and CPUs" and
        "programmer responsibility for synchronization constructs".

	Distributed memory
		It consists in processors (each operate independently) with its own
		memories connected across a communication network. This network exists
		for a processor access the other's memory and is, relatively, a common
		characteristic between distributed and shared memory architectures.

		As each processor has it own local memory, there is no concept of global
		address space across all processors and no application for the concept
		of the cache coherency. A change that a processor makes at his local
		memory doesn't affect the others processors's memories.

		Advantages:
			- the memory increases as the number of processors increases;
			- each processor has quick access to it own memory without
			interference and overhead as there isn't a global cache coherency to
			maintain;
			- the nodes of the distributed memory system can be off-the-shelf
			processors with a network (that can be simple as Ethernet)
			connecting them - cost effectiveness;

		Disadvantages:
			- it's programmer's responsability to take care of data
			communications and synchronization between tasks;
			- existingdata structures that are based on global memory may be
			difficult to map;
			- memory access times are non-uniform, as "data residing in a remote
			node takes longer to access than node local data".

	Hybrid Distributed-Shared Memory
		The top computers of the present use this architecture, that consists of
		shared-memory machines (or a graphics processing unit - GPU) connected
		across a network, that's needed to move data from onde machine to
		another, as each machine can only access its own memory. Apparently,
		this architecture "will continue to prevail and increase at the high end
		of computing for the foreseeable future."

		Advantages and Disadvantages:
		- Whatever is common to both shared and distributed memory architectures.
		- Increased scalability is an important advantage
		- Increased programmer complexity is an important disadvantage

Parallel Programming Models (PPM)
	Existing as a abstraction of the hardware and memory hierarchy, the
	subsequent models are not specific for a type of machine, being
	(theoretically) possible to implement in any hardware. Also, there is no
	best model, but there are best implementations for each one.

	Shared Memory Model (without threads)
		Being, maybe, the simplest PPM, it consists in processes/tasks sharing a
        common address space, which they can write/read assynchronously. It uses
        locks/semaphores "to control access to the shared memory, resolve
        contentions and to prevent race conditions and deadlocks(*)".

        As an advantage, this PPM hasn't the concept of "ownership". In this
        way, there's no need to define explicitily how the data should be
        communicated between the tasks. In counterpart, as an disadvantage of
        the programming model, it's to control data locality(*).

    Threads model
        Being a type of shared memory programming, it's based on a "single
        heavy weight process" with many "light weight" concurrent execution
        paths, called threads. These threads can be scheduled and executed
        simultaneously (concurrency).

        So, you have a main function/process/program that calls another ones and
        give them to threads, that have the incredible power of doing the tasks
        simultaneously.

        POSIX Threads
            Specified by the IEEE POSIX 1003.1c standard (1995). C Language only.
            Part of Unix/Linux operating systems
            Library based
            Commonly referred to as Pthreads.
            Very explicit parallelism; requires significant programmer attention
                to detail.
        OpenMP
            Industry standard, jointly defined and endorsed by a group of major
                computer hardware and software vendors, organizations and
                individuals
            Compiler directive based
            Portable / multi-platform, including Unix and Windows platforms
            Available in C/C++ and Fortran implementations
            Can be very easy and simple to use - provides for "incremental
                parallelism". Can begin with serial code.

    Distributed memory / Message Passing Model
        Consists on tasks that can reside on the same physical address and/or
        across an arbitrary number of machines. Each task uses its own local
        memory during computation.

        These task can need to send/receive data from another task, what can be
        done through sending/receiving messages. These data operations usually
        requires cooperative operations, like an sending subroutine having as a
        pair an equivalent receiving subroutine.

    Data model (Partitioned Global Address Space [PGAS])
        This model consists of focusing parallel work in a dataset, being this
        one disponible for all tasks involved in the system. Each task is,
        generaly, suposed to execute the same operations in a part of the
        dataset.

        Also, the Data Model has address space treated globally. So, Yet, in
        distributed memory architectures,

    Hybrid model
        It combines more than one model presented here. Commons examples are
        (considering multi/many core-machines):
            - MPI and OpenMP: the nodes use OpenMP for dividing the work on
            local computations. The system does the communication between nodes
            using MPI and network;
            - MPI and Pthreads: the nodes use Pthreads for dividing the work on
            local computations using created threads. The system does the
            communication between nodes using MPI and network.

    SMPD and MPMD
        SMPD (Single Program Multiple Data) is model based on each task involved
        having a copy of the program and executing it (or a part of it) on
        multiple data, being a different data for each task. MPMD (Multiple
        Program Multiple Data) is another model based on each task involved
        having a different program to execute on a different dataset.

Designing Parallel Programs
    Automatic vs. Manual Parallelization
        In the most of cases, parallelism is done by the programmer in a
        "time consuming, complex, error-prone and iterative process". However,
        the parallelization task can be done automatically by a compiler or
        pre-processor in a full automatic way, where the compiler analyzes the
        code and identifies the parts of it that can be parallelized, or by a
        programmer directed way, where the programmer set flags or compiler
        directives for indicate paralleling parts of the code to the compiler.

    Understand the Problem and the Program
        Developing parallel software requires understanding of the problem to be
        solved. Understand the problem will lead to the conclusion of if the
        problem is parallelizable or not.

        Also, if you are working with a serial program, understanding the code
        will be necessary. It includes identifying hotspots, that are the points
        of the code that do most of the work and that parallelization will lead
        to bigger results, and the bottlenecks, that are places that execution
        slow down desproportionaly or parallel work gets inefficient. In the
        last case, the solution can be restructurating the program, adopting
        another algorithm that reduce the slowness, or eliminating the
        unnecessary slow areas.

    Partitioning
        When parallelizing, the first step is to break the problem into discrete
        parts that can be distributed to tasks. This process is know as
        partitioning or decomposition. There are two ways of doing this process:
        - domain partitioning: the dataset is decomposed in parts and each part
        gets a subdataset to work on it.
        - functional partitioning: the focus is on the work (and its parts) to
        be done. Each task gets a part of the overall work.

    Communications

    Synchronization
        - "Managing the sequence of work and the tasks performing it is a
        critical design consideration for most parallel programs."
        - "Can be a significant factor in program performance (or lack of it)"
        - Types of synchronization
            - barrier
                It consists on each task working until reach the barrier, when
                they stop, get blocked. When the last task reach the barrier,
                the process is synchronized. What happens then varies.
            - lock/semaphore
            - synchronous communication operations

    Data dependencies

    Load balancing

    Granularity

    I/O

    Debugging

    Performance analysis and tuning


Source: https://computing.llnl.gov/tutorials/parallel_comp/
        Last access: 19-february-2019
