Parallel Computing - LLNL - Just an abstract

Serial computation
    By tradition, we used to write programs to solve problems by a serial 
    (sequential) way for execution in a single processor. It means, the 
    instructions of these programs were executed by one at time.
Parallel computation
    In the parallel approach, we get the problem to be solved and separate 
    it in $n$ parts. So, we get this parts and give to $n$ different 
    processors. For this, we employ a control/coordination mechanism. It 
    often takes less time to solve the given problem in comparison with 
    the serial approach.

The parallel approach needs 
    - compatible resources (parallel computers, computers connected in a 
      network, etc); 
    - a compatible problem (that can be broken in parts, get multiple 
        instructions executed simutaneously, etc).

Why use parallel computing?
    - "Compared to serial computing, parallel computing is much better suited 
      for modeling, simulating and understanding complex, real world phenomena."
    - "save time and/or money"
    - "solve larger/more complex problems"
    - "do many things simultaneously"
    - better usage of non-local resources and parallel hardware

The von Neumann Architecture
    Create by the hungarian mathematician Jonh von Neumann, it was designed 
    to keep data and program instructions in a electronic memory, instead of 
    a scheme of "hard wiring" of the earlier computers. Because of this 
    architecture, the subsequent computers from 1945 (year of publication of 
    von Neumann Architecture) to today have as componnents (but not just these)
        - memory (stores program instructions and data);
        - control unit (operates above instructions, fetching, decodifying and 
            coodinating);
        - arithmetic logic unit (that performs basic arithmetic operations);
        - input/output (interface to human operators).
    "(...) Parallel computers still follow this basic design, just multiplied 
    in units. The basic, fundamental architecture remains the same."

The Flynn's Taxonomy
    Used to classify parallel computers since 1966 by "two independent dimensions 
    of Instruction Stream and Data Stream", each one with only two possible 
    stages: Single and Multiple.
        - SISD (Single Instruction, Single Data)
            One instruction being executed at CPU during one clock cycle and 
            one data stream being used as input any one sigle clock cycle, like 
            in a serial computer, with deterministic execution
        - SIMD (Single Instruction, Multiple Data)
        - MISD (Multiple Instruction, Single Data)
        - MIMD (Multiple Instruction, Multiple Data)
   
   
   
Source: https://computing.llnl.gov/tutorials/parallel_comp/
