Parallel Computing - LLNL - Just an abstract

Serial computation
    By tradition, we used to write programs to solve problems by a serial 
    (sequential) way for execution in a single processor. It means, the 
    instructions of these programs were executed by one at time.
Parallel computation
    In the parallel approach, we get the problem to be solved and separate 
    it in $n$ parts. So, we get this parts and give to $n$ different 
    processors. For this, we employ a control/coordination mechanism. It 
    often takes less time to solve the given problem in comparison with 
    the serial approach.

The parallel approach needs 
    - compatible resources (parallel computers, computers connected in a 
      network, etc); 
    - a compatible problem (that can be broken in parts, get multiple 
        instructions executed simutaneously, etc).

Why use parallel computing?
    - "Compared to serial computing, parallel computing is much better suited 
      for modeling, simulating and understanding complex, real world phenomena."
    - "save time and/or money"
    - "solve larger/more complex problems"
    - "do many things simultaneously"
    - better usage of non-local resources and parallel hardware

The von Neumann Architecture
    Create by the hungarian mathematician Jonh von Neumann, it was designed 
    to keep data and program instructions in a electronic memory, instead of 
    a scheme of "hard wiring" of the earlier computers. Because of this 
    architecture, the subsequent computers from 1945 (year of publication of 
    von Neumann Architecture) to today have as componnents (but not just these)
        - memory (stores program instructions and data);
        - the processor (with the next modules as part of this)
            - control unit (operates above instructions, fetching, decodifying 
            and coodinating);
            - arithmetic logic unit (that performs basic arithmetic operations);
        - input/output (interface to human operators).
    "(...) Parallel computers still follow this basic design, just multiplied 
    in units. The basic, fundamental architecture remains the same."

The Flynn's Taxonomy
    Used to classify parallel computers since 1966 by "two independent 
    dimensions of Instruction Stream and Data Stream", each one with only two 
    possible stages: Single and Multiple.
        - SISD (Single Instruction, Single Data) (non-parallel)
            One instruction being executed at CPU during one clock cycle and 
            one data stream being used as input any one sigle clock cycle, like 
            in a serial computer, with deterministic execution
        - SIMD (Single Instruction, Multiple Data)
            It executes by a synchronous and derteministic way, where all 
            processing units execute the same instruction, but each one can use 
            a different data element
        - MISD (Multiple Instruction, Single Data)
            It works by each processing unit using using the same data via 
            separated instruction streams.
        - MIMD (Multiple Instruction, Multiple Data)
            Being the most common parallel archtecture in these days, it can be 
            built by others SIMD modules. In the model, each processing unit can 
            use different instructions and data streams from the others.

Some General Parallel Terminology (just the unknow or most important)
    "CPUs with multiple cores are sometimes called 'sockets'"
    Task
        "A logically discrete section of computational work. A task is typically 
        a program or program-like set of instructions that is executed by a 
        processor. A parallel program consists of multiple tasks running on 
        multiple processors."
    Pipelining (different meaning from the commom computer architecture)
        "Breaking a task into steps performed by different processor units, with 
        inputs streaming through, much like an assembly line; a type of parallel 
        computing."
    Shared Memory
        "From a strictly hardware point of view, describes a computer 
        architecture where all processors have direct (usually bus based) access 
        to common physical memory. In a programming sense, it describes a model 
        where parallel tasks all have the same "picture" of memory and can 
        directly address and access the same logical memory locations regardless 
        of where the physical memory actually exists."
    Symmetric Multi-Processor (SMP)
        "Shared memory hardware architecture where multiple processors share a 
        single address space and have equal access to all resources."
    Distributed Memory
        "In hardware, refers to network based memory access for physical memory 
        that is not common. As a programming model, tasks can only logically 
        "see" local machine memory and must use communications to access memory 
        on other machines where other tasks are executing."
    Communications
        "Parallel tasks typically need to exchange data. There are several ways 
        this can be accomplished, such as through a shared memory bus or over a 
        network, however the actual event of data exchange is commonly referred 
        to as communications regardless of the method employed."
    Parallel Overhead
        "The amount of time required to coordinate parallel tasks, as opposed to 
        doing useful work. Parallel overhead can include factors such as:
            Task start-up time
            Synchronizations
            Data communications
            Software overhead imposed by parallel languages, libraries, 
            operating system, etc.
            Task termination time"
    Synchronization
	    "The coordination of parallel tasks in real time, very often associated 
	    with communications. Often implemented by establishing a synchronization
        point within an application where a task may not proceed further until 
        another task(s) reaches the same or logically equivalent point.
        Synchronization usually involves waiting by at least one task, and can 
        therefore cause a parallel application's wall clock execution time to
        increase."
    Granularity
	    "In parallel computing, granularity is a qualitative measure of the 
	    ratio of computation to communication.
		    - Coarse: relatively large amounts of computational work are done 
		    between communication events
		    - Fine: relatively small amounts of computational work are done 
		    between communication events"

    Observed Speedup
        "Observed speedup of a code which has been parallelized, defined as:
            wall-clock time of serial execution
            -----------------------------------
           wall-clock time of parallel execution
        One of the simplest and most widely used indicators for a parallel 
        program's performance."

    Massively Parallel
        "Refers to the hardware that comprises a given parallel system - having 
        many processing elements. The meaning of "many" keeps increasing, but
        currently, the largest parallel computers are comprised of processing 
        elements numbering in the hundreds of thousands to millions."
    Embarrassingly Parallel
        "Solving many similar, but independent tasks simultaneously; little to 
        no need for coordination between the tasks."
    Scalability
        "Refers to a parallel system's (hardware and/or software) ability to 
        demonstrate a proportionate increase in parallel speedup with the 
        addition of more resources. Factors that contribute to scalability 
        include:
            - Hardware - particularly memory-cpu bandwidths and network 
            communication properties
            - Application algorithm
            - Parallel overhead related
            - Characteristics of your specific application"

Limits and costs of parallel programming
    Amdahl's Law 
        It intents to measure the speedup involved in a program that 
        has a porcentage $P$ of it parallelized by the formula
                                            1
                                speedup = -----
                                          1 - P
        We can calculate the speedup also involving the number $N$ of processors 
        that can execute the parallel part of code simutaneously, excepting the 
        serial porcentage $S$ of code.
                                            1
                                speedup = -----
                                          P - S
                                         ---
                                          N  
        The speedup by scalability has a limit. But this limit can be increased 
        by increasing the problem size (for some problems). This type of problem 
        is more scalable than the problems "with a fixed percentage of parallel 
        time".
        
    Complexity
        "In general, parallel applications are much more complex than 
        corresponding serial applications, perhaps an order of magnitude. Not 
        only do you have multiple instruction streams executing at the same 
        time, but you also have data flowing between them."

    Resource Requirements
        "For short running parallel programs, there can actually be a decrease 
        in performance compared to a similar serial implementation. The 
        overhead costs associated with setting up the parallel environment, 
        task creation, communications and task termination can comprise a 
        significant portion of the total execution time for short runs."
    
    Scalability
        Strong scaling
            Run a program faster increasing the number of processors that work 
            with this
        Weak scaling
            As the problem size increases, the problem is solved with constant 
            time. This is, you give to all the processors involved the same 
            amount of work and the problem will run always in the same amount 
            of time (at least, in theory)
        As said above, the scalability has limits and going over this limits 
        can result in decreasing of the performance.
        
Parallel Computer Memory Architectures
    Shared memory
        Consist in processors that can operate independently but sharing the 
        same memory resources. It can be classified in
            - Uniform Memory Access (UMA): with identical processors, each of 
            this has equal access and access time to the memory;
            - Non-uniform Memory Access (NUMA): generaly, it is built by UMAs 
            linked, in a way that a UMA can access memory of other (this type 
            of access is slower). In this one, "not all the processors have 
            equal access time to all memories".
            
        - Advantages: "user-friendly programming perspective to memory" and fast
        and uniform data-sharing between tasks.
        - Disadvantages: "lack of scalability between memory and CPUs" and 
        "programmer responsibility for synchronization constructs".
     
    
Source: https://computing.llnl.gov/tutorials/parallel_comp/
        Last access: 01-august-2018
