Parallel Computing - LLNL - Just an abstract

Serial computation
    By tradition, we used to write programs to solve problems by a serial 
    (sequential) way for execution in a single processor. It means, the 
    instructions of these programs were executed by one at time.
Parallel computation
    In the parallel approach, we get the problem to be solved and separate 
    it in $n$ parts. So, we get this parts and give to $n$ different 
    processors. For this, we employ a control/coordination mechanism. It 
    often takes less time to solve the given problem in comparison with 
    the serial approach.

The parallel approach needs 
    - compatible resources (parallel computers, computers connected in a 
      network, etc); 
    - a compatible problem (that can be broken in parts, get multiple 
        instructions executed simutaneously, etc).

Why use parallel computing?
    - "Compared to serial computing, parallel computing is much better suited 
      for modeling, simulating and understanding complex, real world phenomena."
    - "save time and/or money"
    - "solve larger/more complex problems"
    - "do many things simultaneously"
    - better usage of non-local resources and parallel hardware

The von Neumann Architecture
    Create by the hungarian mathematician Jonh von Neumann, it was designed 
    to keep data and program instructions in a electronic memory, instead of 
    a scheme of "hard wiring" of the earlier computers. Because of this 
    architecture, the subsequent computers from 1945 (year of publication of 
    von Neumann Architecture) to today have as componnents (but not just these)
        - memory (stores program instructions and data);
        - the processor (with the next modules as part of this)
            - control unit (operates above instructions, fetching, decodifying 
            and coodinating);
            - arithmetic logic unit (that performs basic arithmetic operations);
        - input/output (interface to human operators).
    "(...) Parallel computers still follow this basic design, just multiplied 
    in units. The basic, fundamental architecture remains the same."

The Flynn's Taxonomy
    Used to classify parallel computers since 1966 by "two independent 
    dimensions of Instruction Stream and Data Stream", each one with only two 
    possible stages: Single and Multiple.
        - SISD (Single Instruction, Single Data) (non-parallel)
            One instruction being executed at CPU during one clock cycle and 
            one data stream being used as input any one sigle clock cycle, like 
            in a serial computer, with deterministic execution
        - SIMD (Single Instruction, Multiple Data)
            It executes by a synchronous and derteministic way, where all 
            processing units execute the same instruction, but each one can use 
            a different data element
        - MISD (Multiple Instruction, Single Data)
            It works by each processing unit using using the same data via 
            separated instruction streams.
        - MIMD (Multiple Instruction, Multiple Data)
            Being the most common parallel archtecture in these days, it can be 
            built by others SIMD modules. In the model, each processing unit can 
            use different instructions and data streams from the others.

Some General Parallel Terminology (just the unknow or most important)
    "CPUs with multiple cores are sometimes called 'sockets'"
    Task
        "A logically discrete section of computational work. A task is typically 
        a program or program-like set of instructions that is executed by a 
        processor. A parallel program consists of multiple tasks running on 
        multiple processors."
    Pipelining (different meaning from the commom computer architecture)
        "Breaking a task into steps performed by different processor units, with 
        inputs streaming through, much like an assembly line; a type of parallel 
        computing."
    Shared Memory
        "From a strictly hardware point of view, describes a computer 
        architecture where all processors have direct (usually bus based) access 
        to common physical memory. In a programming sense, it describes a model 
        where parallel tasks all have the same "picture" of memory and can 
        directly address and access the same logical memory locations regardless 
        of where the physical memory actually exists."
    Symmetric Multi-Processor (SMP)
        "Shared memory hardware architecture where multiple processors share a 
        single address space and have equal access to all resources."
    Distributed Memory
        "In hardware, refers to network based memory access for physical memory 
        that is not common. As a programming model, tasks can only logically 
        "see" local machine memory and must use communications to access memory 
        on other machines where other tasks are executing."
    Communications
        "Parallel tasks typically need to exchange data. There are several ways 
        this can be accomplished, such as through a shared memory bus or over a 
        network, however the actual event of data exchange is commonly referred 
        to as communications regardless of the method employed."
    Parallel Overhead
        The amount of time required to coordinate parallel tasks, as opposed to 
        doing useful work. Parallel overhead can include factors such as:
            Task start-up time
            Synchronizations
            Data communications
            Software overhead imposed by parallel languages, libraries, 
            operating system, etc.
            Task termination time
    
    
Source: https://computing.llnl.gov/tutorials/parallel_comp/
        Last access: 31-july-2018
